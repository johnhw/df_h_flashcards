parameters in optimization; variables that the optimization algorithm adjusts to find the best solution
objective function; a mathematical function representing the quantity to be optimized (maximized or minimized)
roles of objective function; measures the performance of the solution and guides the optimization process
equality constraints; constraints that require specific conditions to be met exactly in the optimization problem
inequality constraints; constraints that define conditions that must be satisfied in the optimization problem, but not necessarily exactly
penalisation; technique to handle constraints by adding penalty terms to the objective function for violating constraints
penalty function; a mathematical function that quantifies the violation of constraints or undesirable parameter values, influencing the optimization process
approximation form of optimization; optimization problems where the objective function is an approximation, often used in machine learning tasks
penalty parameter; a tuning parameter (usually $\lambda$) that controls the strength of penalization, balancing between fitting the data and penalizing the complexity of the model
approximation form of optimization; $$L(\theta) = \sum \| y_i - f(x_i, \theta) \|$$ represents the approximation form of optimization where it minimizes the sum of absolute differences between observed and predicted values
differentiability in optimization; property of a function that allows it to have derivatives, crucial for gradient-based optimization algorithms
local optimum; a solution that is optimal within a specific neighborhood, but not necessarily the best solution globally
global optimum; the best possible solution across the entire feasible region of an optimization problem
convexity in optimization; a property of the objective function where any line segment connecting two points on the graph lies above the graph, equivalent to a single global optimum
saddle point; a point in the objective function where it is neither a local minimum nor a local maximum
linear regression in optimization framework; modeling linear regression as an optimization problem to find the best-fitting line for given data points
convex sums; technique of aggregating multiple objective functions into a single objective function by taking a weighted sum
heuristic optimization; a trial-and-error approach to problem-solving, using rules of thumb and intuition to find approximate solutions. No guarantee of optimality
convergence graphs; graphical representation showing how the optimization algorithm's objective function value against iteration - a key diagnostic tool for optimization
grid search; exhaustive search method that becomes computationally expensive with high-dimensional parameter spaces
hyperparameters vs parameters; parameters are optimized during the learning process, while hyperparameters are set to tune or configure the optimisation process itself
random search in optimization; an optimization technique that explores the search space randomly, suitable for high-dimensional problems
stochastic hill climbing; iterative optimization algorithm that makes small random steps in the search space to find the optimal solution
genetic algorithms in optimization; optimization technique that mimics the process of natural selection and genetic evolution to find optimal solutions
mutation; mutation introduces diversity in genetic algorithms by adding noise (as in stochastic hill-climbing)
crossover; combines genetic material (parameter vectors) from two parents (other solutions) to create offspring
convexity; a property where the objective function forms a convex shape, allowing for efficient optimization. There is a single global optimum.
convex sums in optimization; combining multiple convex objective functions to create a convex overall objective function
convex functions; functions where the line segment between any two points on the graph lies above the graph itself
concave functions; functions where the line segment between any two points on the graph lies below the graph itself
convex combination; a combination of vectors where the weights are non-negative and sum up to 1
continuity; a property of functions where small changes in the input result in small changes in the output
discontinous; a property of functions where arbitarily small changes in the input can result in arbitarily large changes in the output
C^0 continuity; a function is continuous, but not necessarily differentiable (has no gaps or jumps in the graph)
C^1 continuity; a function is continuously differentiable (has a continuous first derivative)
C^2 continuity; a function has a continuous first and second derivative
C^∞ continuity; a function has every derivative continuous
differentiability in optimization; a property of functions where the derivative at a point exists, indicating the rate of change or gradient
continuity and differentiability relationship; all differentiable functions are continuous, but not all continuous functions are differentiable
functions that are continuous but not differentiable; examples include the absolute value function and the square root function
functions that are both continuous and differentiable; examples include linear functions, sine, cosine, x^2
functions that are neither continuous nor differentiable; examples include step functions 
discontinuous function example; $$f(x) = 1/x$$ (reciprocal function) -- discontinuous at x = 0
C^0 continuous function example; $$f(x) = |x|$$ (absolute value function) -- derivative is undefined at x = 0
C^1 continuous function example; $$f(x) = x^2$$ (quadratic function)
C^2 continuous function example; $$f(x) = 3x^3 - 2x^2 + 5x - 1$$ (cubic function)
C^∞ (infinitely differentiable) continuous function example; f(x) = e^x (exponential function)
temperature; a metaheuristic, controlling the likelihood of accepting worse solutions to escape local optima
memory; a metaheuristic where an optimisation algorithm remembers past solutions and use them to guide the search process
stigmergy; a communication mechanism where agents indirectly interact with each other through modifications of their environment (population + memory)
population; a group of potential solutions or individuals in algorithms like genetic algorithms and particle swarm optimization
genetic algorithms; optimization algorithms inspired by the process of natural selection, involving crossover and mutation operations. Hill climbing + population.
simulated annealing; probabilistic optimization algorithm inspired by the annealing process in metallurgy, allowing for exploration of the solution space. Hill climbing + temperature.
temperature schedule; a function that decreases the temperature over time, controlling the annealing process in simulated annealing.
hyperparameters of grid search; grid spacing, the range of values for each parameter
hyperparameters of stochastic hill climbing; step distribution (or step size)
hyperparameters of genetic algorithms; population size, crossover probability, mutation probability
general objective function in optimization; f(x, θ) represents the function to be minimized or maximized, where x is the input and θ are the parameters
approximation form of optimization; L(θ) = Σ |y_i - f(x_i, θ)| represents the objective function as the sum of absolute differences between observed (y_i) and predicted (f(x_i, θ)) values
parameter vector;an element in a vector space representing a set of parameters, often used in mathematical modeling and machine learning
Euclidean space R^n;a vector space representing n-dimensional real-valued vectors, commonly used to represent parameter vectors in various contexts
